{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 6833925,
          "sourceType": "datasetVersion",
          "datasetId": 3928988
        },
        {
          "sourceId": 6868302,
          "sourceType": "datasetVersion",
          "datasetId": 3947111
        },
        {
          "sourceId": 13379298,
          "sourceType": "datasetVersion",
          "datasetId": 8488650
        },
        {
          "sourceId": 13657025,
          "sourceType": "datasetVersion",
          "datasetId": 8682619
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "tutorÃ©",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "xiaoweixumedicalai_chd68_segmentation_dataset_miccai19_path = kagglehub.dataset_download('xiaoweixumedicalai/chd68-segmentation-dataset-miccai19')\n",
        "xiaoweixumedicalai_imagechd_path = kagglehub.dataset_download('xiaoweixumedicalai/imagechd')\n",
        "azizesseghaier_parse_challenge_2022_training_set_path = kagglehub.dataset_download('azizesseghaier/parse-challenge-2022-training-set')\n",
        "azizesseghaier_image_chd_pa_only_labels_path = kagglehub.dataset_download('azizesseghaier/image-chd-pa-only-labels')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "IQaN955UNabF"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip the image chd dataset and keep only the pulmonary arteries labels"
      ],
      "metadata": {
        "id": "dJpXQzK8NabI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T17:01:22.463037Z",
          "iopub.execute_input": "2025-11-08T17:01:22.463348Z",
          "iopub.status.idle": "2025-11-08T17:01:23.499291Z",
          "shell.execute_reply.started": "2025-11-08T17:01:22.463316Z",
          "shell.execute_reply": "2025-11-08T17:01:23.498168Z"
        },
        "id": "WXpXOHtvNabJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "MPpiisjBNabK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"imagechd\", exist_ok=True)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T17:01:23.500975Z",
          "iopub.execute_input": "2025-11-08T17:01:23.501641Z",
          "iopub.status.idle": "2025-11-08T17:01:23.507751Z",
          "shell.execute_reply.started": "2025-11-08T17:01:23.501613Z",
          "shell.execute_reply": "2025-11-08T17:01:23.506337Z"
        },
        "id": "4PS9L2A9NabK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.change2zip /kaggle/working/imagechd/ImageCHD_dataset.zip\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z01 /kaggle/working/imagechd/ImageCHD_dataset.z01\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z02 /kaggle/working/imagechd/ImageCHD_dataset.z02\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z03 /kaggle/working/imagechd/ImageCHD_dataset.z03\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z04 /kaggle/working/imagechd/ImageCHD_dataset.z04\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z05 /kaggle/working/imagechd/ImageCHD_dataset.z05\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z06 /kaggle/working/imagechd/ImageCHD_dataset.z06\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z07 /kaggle/working/imagechd/ImageCHD_dataset.z07\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z08 /kaggle/working/imagechd/ImageCHD_dataset.z08\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z09 /kaggle/working/imagechd/ImageCHD_dataset.z09\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z10 /kaggle/working/imagechd/ImageCHD_dataset.z10\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z11 /kaggle/working/imagechd/ImageCHD_dataset.z11\n",
        "!mv /kaggle/input/imagechd/ImageCHD_dataset.z12 /kaggle/working/imagechd/ImageCHD_dataset.z12"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T17:01:23.508686Z",
          "iopub.execute_input": "2025-11-08T17:01:23.508956Z",
          "iopub.status.idle": "2025-11-08T17:02:30.344887Z",
          "shell.execute_reply.started": "2025-11-08T17:01:23.508933Z",
          "shell.execute_reply": "2025-11-08T17:02:30.343469Z"
        },
        "id": "lEDv1sQWNabL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#!cat /kaggle/working/chd68-segmentation-dataset-miccai19/CHD68_segmentation_dataset_miccai19.z* /kaggle/working/chd68-segmentation-dataset-miccai19/CHD68_segmentation_dataset_miccai19.zip > /kaggle/working/full_dataset.zip"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T17:02:30.34755Z",
          "iopub.execute_input": "2025-11-08T17:02:30.347878Z",
          "iopub.status.idle": "2025-11-08T17:02:30.353584Z",
          "shell.execute_reply.started": "2025-11-08T17:02:30.347851Z",
          "shell.execute_reply": "2025-11-08T17:02:30.352182Z"
        },
        "id": "5giIX2HeNabL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y p7zip-full\n",
        "!7z x /kaggle/working/imagechd/ImageCHD_dataset.zip -o/kaggle/working/unzipped"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T17:02:30.354765Z",
          "iopub.execute_input": "2025-11-08T17:02:30.355115Z",
          "iopub.status.idle": "2025-11-08T17:03:33.957316Z",
          "shell.execute_reply.started": "2025-11-08T17:02:30.35509Z",
          "shell.execute_reply": "2025-11-08T17:03:33.95602Z"
        },
        "id": "D7Bd8Kq8NabL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /kaggle/working/imagechd"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T17:03:33.958548Z",
          "iopub.execute_input": "2025-11-08T17:03:33.958816Z",
          "iopub.status.idle": "2025-11-08T17:03:35.145622Z",
          "shell.execute_reply.started": "2025-11-08T17:03:33.95879Z",
          "shell.execute_reply": "2025-11-08T17:03:35.144157Z"
        },
        "id": "eGgJZpHiNabM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting all possible labels from the image chd dataset"
      ],
      "metadata": {
        "id": "VZrh7ru_NabM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting all possible labels and their data types\n",
        "\n",
        "# Directory containing your label files\n",
        "source_dir = Path(\"/kaggle/working/unzipped/ImageCHD_dataset\")\n",
        "\n",
        "data_type_set = set()\n",
        "labels_set = set()\n",
        "\n",
        "# Loop over all files ending with _label.nii or _label.nii.gz\n",
        "for label_path in sorted(source_dir.glob(\"*_label.nii*\")):\n",
        "    # Skip files starting with ._\n",
        "    if label_path.name.startswith(\"._\"):\n",
        "        continue\n",
        "\n",
        "    print(\"Processing:\", label_path.name)\n",
        "    img = nib.load(label_path)\n",
        "    data = img.get_fdata()\n",
        "\n",
        "    # Collect info\n",
        "    labels = np.unique(data)\n",
        "    data_type_set.add(data.dtype)\n",
        "    labels_set = labels_set.union(set(labels))\n",
        "\n",
        "print(\"Data types found:\", data_type_set)\n",
        "print(\"All labels found:\", labels_set)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-31T11:29:34.556191Z",
          "iopub.execute_input": "2025-10-31T11:29:34.557572Z",
          "iopub.status.idle": "2025-10-31T11:39:10.293536Z",
          "shell.execute_reply.started": "2025-10-31T11:29:34.557501Z",
          "shell.execute_reply": "2025-10-31T11:39:10.291792Z"
        },
        "id": "7hdXEOrPNabM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keeping the pulmonary arteries labels"
      ],
      "metadata": {
        "id": "tGSou1TCNabM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Directory containing both images and labels\n",
        "data_dir = Path(\"/kaggle/working/unzipped/ImageCHD_dataset\")\n",
        "\n",
        "# Output directory\n",
        "output_dir = Path(\"/kaggle/working/image_chd_pa_only_labels/\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "PATHS = sorted(data_dir.glob(\"*_label.nii*\"))\n",
        "# Loop over all label files\n",
        "for label_path in tqdm(PATHS, desc=\"processing labels\"):\n",
        "    # Skip hidden files\n",
        "    if label_path.name.startswith(\"._\"):\n",
        "        continue\n",
        "\n",
        "    print(\"Processing:\", label_path.name)\n",
        "\n",
        "    # Load label file\n",
        "    label_img = nib.load(label_path)\n",
        "    label_data = label_img.get_fdata()\n",
        "\n",
        "    # Extract pulmonary artery (label 7) and convert to int8\n",
        "    pulmonary_only = np.where(label_data == 7., 1.0, 0.0)\n",
        "\n",
        "    # Skip saving if pulmonary artery not present\n",
        "    if np.max(pulmonary_only) == 0:\n",
        "        print(\"No pulmonary artery label found, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Create new NIfTI image for pulmonary label\n",
        "    new_label_img = nib.Nifti1Image(pulmonary_only, label_img.affine, label_img.header)\n",
        "    label_out_path = output_dir / label_path.name\n",
        "    nib.save(new_label_img, label_out_path)\n",
        "\n",
        "print(\"Done!!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T11:56:52.178568Z",
          "iopub.execute_input": "2025-11-08T11:56:52.181942Z",
          "iopub.status.idle": "2025-11-08T11:59:59.805355Z",
          "shell.execute_reply.started": "2025-11-08T11:56:52.181851Z",
          "shell.execute_reply": "2025-11-08T11:59:59.804026Z"
        },
        "id": "Bym_elu7NabM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#! zip -r labels.zip /kaggle/working/image_chd_pa_only_labels"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T12:19:05.965189Z",
          "iopub.execute_input": "2025-11-08T12:19:05.967158Z",
          "iopub.status.idle": "2025-11-08T12:19:07.124627Z",
          "shell.execute_reply.started": "2025-11-08T12:19:05.967107Z",
          "shell.execute_reply": "2025-11-08T12:19:07.123289Z"
        },
        "id": "ygRryJfiNabN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking all possible labels in the parse dataset"
      ],
      "metadata": {
        "id": "8ELEU9GjNabN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "\n",
        "img = nib.load(\"/kaggle/input/parse-challenge-2022-training-set/train/train/PA000036/label/PA000036.nii\")\n",
        "data = img.get_fdata()\n",
        "labels = np.unique(data)\n",
        "labels_set = set(labels)\n",
        "labels_set"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-01T20:47:46.84707Z",
          "iopub.execute_input": "2025-11-01T20:47:46.847384Z",
          "iopub.status.idle": "2025-11-01T20:47:54.726424Z",
          "shell.execute_reply.started": "2025-11-01T20:47:46.847361Z",
          "shell.execute_reply": "2025-11-01T20:47:54.725312Z"
        },
        "id": "kJhVfnpUNabN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "all_poosible_labels_CHD68 = {0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 11.0, 12.0, 13.0, 14.0, 15.0}"
      ],
      "metadata": {
        "trusted": true,
        "id": "aqznXVXxNabN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "all_possible_labels_imageCHD = {0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 10.0, 12.0, 13.0, 14.0, 15.0}"
      ],
      "metadata": {
        "trusted": true,
        "id": "Isw70vcDNabN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking for possible duplicates in both datasets"
      ],
      "metadata": {
        "id": "dbhXka5VNabN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Notebook-friendly duplicate checker for quick analysis in Kaggle/Colab\n",
        "\n",
        "Usage in notebook:\n",
        "    %run scripts/check_duplicates_notebook.py\n",
        "\n",
        "    Or import:\n",
        "    from scripts.check_duplicates_notebook import quick_duplicate_check\n",
        "\n",
        "    results = quick_duplicate_check(\n",
        "        parse22_path=\"/kaggle/input/parse-challenge-2022-training-set\",\n",
        "        imagechd_path=\"/kaggle/working/unzipped/ImageCHD_dataset\"\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def find_parse22_images(base_path):\n",
        "    \"\"\"Find PARSE22 images.\"\"\"\n",
        "    base_path = Path(base_path)\n",
        "    images = []\n",
        "\n",
        "    # Try different patterns\n",
        "    for pattern in [\"train/train/*/image/*.nii\", \"**/PA*/image/*.nii\"]:\n",
        "        images.extend(base_path.glob(pattern))\n",
        "\n",
        "    images = list(set(images))\n",
        "    print(f\"âœ“ Found {len(images)} PARSE22 images\")\n",
        "    return images\n",
        "\n",
        "\n",
        "def find_imagechd_images(base_path):\n",
        "    \"\"\"Find ImageCHD images.\"\"\"\n",
        "    base_path = Path(base_path)\n",
        "    images = []\n",
        "\n",
        "    # Try different patterns\n",
        "    for pattern in [\"ct_*_image.nii.gz\", \"**/ct_*_image.nii.gz\"]:\n",
        "        images.extend(base_path.glob(pattern))\n",
        "\n",
        "    images = list(set(images))\n",
        "    print(f\"âœ“ Found {len(images)} ImageCHD images\")\n",
        "    return images\n",
        "\n",
        "\n",
        "def compute_file_hash(file_path):\n",
        "    \"\"\"Compute MD5 hash of file.\"\"\"\n",
        "    md5_hash = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            md5_hash.update(chunk)\n",
        "    return md5_hash.hexdigest()\n",
        "\n",
        "\n",
        "def compute_image_hash(image_array):\n",
        "    \"\"\"Compute hash of image content.\"\"\"\n",
        "    # Normalize and quantize\n",
        "    normalized = (image_array - image_array.min()) / (image_array.max() - image_array.min() + 1e-10)\n",
        "    quantized = (normalized * 1000).astype(np.int16)\n",
        "    return hashlib.md5(quantized.tobytes()).hexdigest()\n",
        "\n",
        "\n",
        "def load_image_info(file_path):\n",
        "    \"\"\"Load image and extract basic info.\"\"\"\n",
        "    try:\n",
        "        img = sitk.ReadImage(str(file_path))\n",
        "        array = sitk.GetArrayFromImage(img)\n",
        "\n",
        "        return {\n",
        "            'path': str(file_path),\n",
        "            'name': file_path.name,\n",
        "            'shape': array.shape,\n",
        "            'spacing': img.GetSpacing(),\n",
        "            'size': img.GetSize(),\n",
        "            'min': float(array.min()),\n",
        "            'max': float(array.max()),\n",
        "            'mean': float(array.mean()),\n",
        "            'std': float(array.std()),\n",
        "            'array': array  # Keep for hash computation\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def check_file_duplicates(parse22_images, imagechd_images):\n",
        "    \"\"\"Check for exact file duplicates.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Checking file hashes (exact file duplicates)...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    hash_map = defaultdict(list)\n",
        "\n",
        "    all_images = parse22_images + imagechd_images\n",
        "\n",
        "    for img_path in tqdm(all_images, desc=\"Computing file hashes\"):\n",
        "        file_hash = compute_file_hash(img_path)\n",
        "        hash_map[file_hash].append(str(img_path))\n",
        "\n",
        "    # Find duplicates\n",
        "    duplicates = {h: files for h, files in hash_map.items() if len(files) > 1}\n",
        "\n",
        "    print(f\"\\nResult: Found {len(duplicates)} groups of file duplicates\")\n",
        "\n",
        "    if duplicates:\n",
        "        print(\"\\nDuplicate groups:\")\n",
        "        for i, (hash_val, files) in enumerate(duplicates.items(), 1):\n",
        "            print(f\"\\n  Group {i} ({len(files)} files):\")\n",
        "            for f in files:\n",
        "                print(f\"    - {Path(f).name}\")\n",
        "\n",
        "    return duplicates\n",
        "\n",
        "\n",
        "def check_content_duplicates(parse22_images, imagechd_images):\n",
        "    \"\"\"Check for content duplicates (same pixel values).\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Checking content hashes (identical pixel values)...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    hash_map = defaultdict(list)\n",
        "    image_info = {}\n",
        "\n",
        "    all_images = parse22_images + imagechd_images\n",
        "\n",
        "    for img_path in tqdm(all_images, desc=\"Loading images and computing hashes\"):\n",
        "        info = load_image_info(img_path)\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        # Compute content hash\n",
        "        content_hash = compute_image_hash(info['array'])\n",
        "        hash_map[content_hash].append(info)\n",
        "        image_info[str(img_path)] = info\n",
        "\n",
        "        # Remove array to save memory\n",
        "        del info['array']\n",
        "\n",
        "    # Find duplicates\n",
        "    duplicates = {h: infos for h, infos in hash_map.items() if len(infos) > 1}\n",
        "\n",
        "    print(f\"\\nResult: Found {len(duplicates)} groups of content duplicates\")\n",
        "\n",
        "    if duplicates:\n",
        "        print(\"\\nDuplicate groups:\")\n",
        "        for i, (hash_val, infos) in enumerate(duplicates.items(), 1):\n",
        "            print(f\"\\n  Group {i} ({len(infos)} images):\")\n",
        "            for info in infos:\n",
        "                print(f\"    - {info['name']}: shape={info['shape']}, mean={info['mean']:.1f}\")\n",
        "\n",
        "    return duplicates, image_info\n",
        "\n",
        "\n",
        "def check_metadata_similarity(image_info):\n",
        "    \"\"\"Check for potential duplicates based on metadata.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Checking metadata similarity...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Group by shape\n",
        "    shape_groups = defaultdict(list)\n",
        "    for path, info in image_info.items():\n",
        "        if 'array' in info:\n",
        "            del info['array']  # Clean up\n",
        "        shape_groups[str(info['shape'])].append(info)\n",
        "\n",
        "    # Find cross-dataset matches\n",
        "    potential_duplicates = []\n",
        "\n",
        "    for shape, images in shape_groups.items():\n",
        "        if len(images) < 2:\n",
        "            continue\n",
        "\n",
        "        # Separate by dataset\n",
        "        parse22 = [img for img in images if 'PA' in img['name']]\n",
        "        imagechd = [img for img in images if 'ct_' in img['name']]\n",
        "\n",
        "        if not parse22 or not imagechd:\n",
        "            continue\n",
        "\n",
        "        # Compare metadata\n",
        "        for p22 in parse22:\n",
        "            for ichd in imagechd:\n",
        "                mean_diff = abs(p22['mean'] - ichd['mean'])\n",
        "                std_diff = abs(p22['std'] - ichd['std'])\n",
        "\n",
        "                # Flag if very similar\n",
        "                if mean_diff < 50 and std_diff < 50:\n",
        "                    potential_duplicates.append({\n",
        "                        'parse22': p22['name'],\n",
        "                        'imagechd': ichd['name'],\n",
        "                        'shape': p22['shape'],\n",
        "                        'mean_diff': mean_diff,\n",
        "                        'std_diff': std_diff,\n",
        "                        'p22_mean': p22['mean'],\n",
        "                        'ichd_mean': ichd['mean']\n",
        "                    })\n",
        "\n",
        "    print(f\"\\nResult: Found {len(potential_duplicates)} potential duplicate pairs\")\n",
        "\n",
        "    if potential_duplicates:\n",
        "        print(\"\\nTop 5 suspicious pairs:\")\n",
        "        for i, dup in enumerate(sorted(potential_duplicates, key=lambda x: x['mean_diff'])[:5], 1):\n",
        "            print(f\"\\n  Pair {i}:\")\n",
        "            print(f\"    PARSE22: {dup['parse22']}\")\n",
        "            print(f\"    ImageCHD: {dup['imagechd']}\")\n",
        "            print(f\"    Mean difference: {dup['mean_diff']:.2f}\")\n",
        "\n",
        "    return potential_duplicates\n",
        "\n",
        "\n",
        "def create_summary_dataframe(image_info):\n",
        "    \"\"\"Create summary DataFrame of all images.\"\"\"\n",
        "    data = []\n",
        "\n",
        "    for path, info in image_info.items():\n",
        "        if 'array' in info:\n",
        "            continue\n",
        "\n",
        "        dataset = 'PARSE22' if 'PA' in info['name'] else 'ImageCHD'\n",
        "        data.append({\n",
        "            'dataset': dataset,\n",
        "            'name': info['name'],\n",
        "            'shape': str(info['shape']),\n",
        "            'spacing': str(info['spacing']),\n",
        "            'min': info['min'],\n",
        "            'max': info['max'],\n",
        "            'mean': info['mean'],\n",
        "            'std': info['std']\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "\n",
        "def quick_duplicate_check(parse22_path, imagechd_path, skip_metadata=False):\n",
        "    \"\"\"\n",
        "    Quick duplicate check - returns results dictionary.\n",
        "\n",
        "    Args:\n",
        "        parse22_path: Path to PARSE22 dataset\n",
        "        imagechd_path: Path to ImageCHD dataset\n",
        "        skip_metadata: Skip metadata similarity check (faster)\n",
        "\n",
        "    Returns:\n",
        "        dict: Results including duplicates and image info\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"QUICK DUPLICATE CHECK\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"PARSE22: {parse22_path}\")\n",
        "    print(f\"ImageCHD: {imagechd_path}\")\n",
        "    print()\n",
        "\n",
        "    # Find images\n",
        "    parse22_images = find_parse22_images(parse22_path)\n",
        "    imagechd_images = find_imagechd_images(imagechd_path)\n",
        "\n",
        "    if not parse22_images or not imagechd_images:\n",
        "        print(\"ERROR: Could not find images in one or both datasets!\")\n",
        "        return None\n",
        "\n",
        "    # Check duplicates\n",
        "    file_duplicates = check_file_duplicates(parse22_images, imagechd_images)\n",
        "    content_duplicates, image_info = check_content_duplicates(parse22_images, imagechd_images)\n",
        "\n",
        "    potential_duplicates = []\n",
        "    if not skip_metadata:\n",
        "        potential_duplicates = check_metadata_similarity(image_info)\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total images: {len(parse22_images) + len(imagechd_images)}\")\n",
        "    print(f\"  PARSE22: {len(parse22_images)}\")\n",
        "    print(f\"  ImageCHD: {len(imagechd_images)}\")\n",
        "    print()\n",
        "    print(f\"Exact file duplicates: {len(file_duplicates)} groups\")\n",
        "    print(f\"Exact content duplicates: {len(content_duplicates)} groups\")\n",
        "    print(f\"Potential duplicates: {len(potential_duplicates)} pairs\")\n",
        "    print()\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    df_summary = create_summary_dataframe(image_info)\n",
        "\n",
        "    # Recommendation\n",
        "    total_exact = len(file_duplicates) + len(content_duplicates)\n",
        "    if total_exact == 0:\n",
        "        print(\"âœ“ No exact duplicates found!\")\n",
        "        print(\"  All images appear to be unique.\")\n",
        "        expected_total = len(parse22_images) + len(imagechd_images)\n",
        "    else:\n",
        "        print(\"âš  Found exact duplicates!\")\n",
        "        print(\"  Recommend removing duplicates before training.\")\n",
        "        # Estimate unique images\n",
        "        dup_count = sum(len(files) - 1 for files in file_duplicates.values())\n",
        "        dup_count += sum(len(infos) - 1 for infos in content_duplicates.values())\n",
        "        expected_total = len(parse22_images) + len(imagechd_images) - dup_count\n",
        "\n",
        "    print(f\"\\nExpected dataset size after cleanup: {expected_total} images\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return {\n",
        "        'parse22_images': parse22_images,\n",
        "        'imagechd_images': imagechd_images,\n",
        "        'file_duplicates': file_duplicates,\n",
        "        'content_duplicates': content_duplicates,\n",
        "        'potential_duplicates': potential_duplicates,\n",
        "        'image_info': image_info,\n",
        "        'summary_df': df_summary,\n",
        "        'total_images': len(parse22_images) + len(imagechd_images),\n",
        "        'expected_unique': expected_total\n",
        "    }\n",
        "\n",
        "\n",
        "# Example usage for Kaggle/Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # Example paths (adjust for your environment)\n",
        "    PARSE22_PATH = \"/kaggle/input/parse-challenge-2022-training-set\"\n",
        "    IMAGECHD_PATH = \"/kaggle/working/unzipped/ImageCHD_dataset\"\n",
        "\n",
        "    # Run check\n",
        "    results = quick_duplicate_check(PARSE22_PATH, IMAGECHD_PATH)\n",
        "\n",
        "    if results:\n",
        "        # Display summary DataFrame\n",
        "        print(\"\\nDataset Summary:\")\n",
        "        print(results['summary_df'].head(10))\n",
        "\n",
        "        # Save results\n",
        "        results['summary_df'].to_csv('dataset_summary.csv', index=False)\n",
        "        print(\"\\nSaved dataset summary to: dataset_summary.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T14:14:52.677764Z",
          "iopub.execute_input": "2025-11-08T14:14:52.678309Z",
          "iopub.status.idle": "2025-11-08T14:29:02.596953Z",
          "shell.execute_reply.started": "2025-11-08T14:14:52.67827Z",
          "shell.execute_reply": "2025-11-08T14:29:02.595113Z"
        },
        "id": "A4mGxPDfNabN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grouping images by dimension"
      ],
      "metadata": {
        "id": "TnNRNsaqNabO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple dimension checker optimized for Kaggle notebooks.\n",
        "Fast, lightweight, and produces easy-to-read output.\n",
        "\n",
        "Copy-paste this entire cell into your Kaggle notebook and run!\n",
        "\"\"\"\n",
        "\n",
        "import SimpleITK as sitk\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def quick_dimension_analysis(parse22_path, imagechd_path):\n",
        "    \"\"\"\n",
        "    Quick dimension analysis - groups images by (D, H, W, C).\n",
        "\n",
        "    Args:\n",
        "        parse22_path: Path to PARSE22 dataset\n",
        "        imagechd_path: Path to ImageCHD dataset\n",
        "\n",
        "    Returns:\n",
        "        dict: Results with dimension groups\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"DATASET DIMENSION ANALYZER\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Find images\n",
        "    print(\"\\nðŸ” Finding images...\")\n",
        "    parse22_images = list(Path(parse22_path).glob(\"**/PA*/image/*.nii*\"))\n",
        "    imagechd_images = list(Path(imagechd_path).glob(\"**/ct_*_image.nii*\"))\n",
        "\n",
        "    print(f\"  PARSE22: {len(parse22_images)} images\")\n",
        "    print(f\"  ImageCHD: {len(imagechd_images)} images\")\n",
        "    print(f\"  Total: {len(parse22_images) + len(imagechd_images)} images\\n\")\n",
        "\n",
        "    # Analyze dimensions\n",
        "    dimension_groups = defaultdict(lambda: {'parse22': [], 'imagechd': [], 'spacing': []})\n",
        "    all_data = []\n",
        "\n",
        "    print(\"ðŸ“Š Analyzing PARSE22...\")\n",
        "    for img_path in tqdm(parse22_images):\n",
        "        try:\n",
        "            img = sitk.ReadImage(str(img_path))\n",
        "            array = sitk.GetArrayFromImage(img)\n",
        "            spacing = img.GetSpacing()\n",
        "\n",
        "            # Get dimensions: array shape is (D, H, W)\n",
        "            d, h, w = array.shape\n",
        "            c = 1  # channels\n",
        "\n",
        "            dim_key = (d, h, w, c)\n",
        "            dimension_groups[dim_key]['parse22'].append(img_path.name)\n",
        "            dimension_groups[dim_key]['spacing'].append(spacing)\n",
        "\n",
        "            all_data.append({\n",
        "                'file': img_path.name,\n",
        "                'dataset': 'PARSE22',\n",
        "                'D': d, 'H': h, 'W': w, 'C': c,\n",
        "                'spacing_x': spacing[0],\n",
        "                'spacing_y': spacing[1],\n",
        "                'spacing_z': spacing[2],\n",
        "                'voxels': d*h*w,\n",
        "                'memory_mb': array.nbytes / (1024**2)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸  Error with {img_path.name}: {e}\")\n",
        "\n",
        "    print(\"\\nðŸ“Š Analyzing ImageCHD...\")\n",
        "    for img_path in tqdm(imagechd_images):\n",
        "        try:\n",
        "            img = sitk.ReadImage(str(img_path))\n",
        "            array = sitk.GetArrayFromImage(img)\n",
        "            spacing = img.GetSpacing()\n",
        "\n",
        "            d, h, w = array.shape\n",
        "            c = 1\n",
        "\n",
        "            dim_key = (d, h, w, c)\n",
        "            dimension_groups[dim_key]['imagechd'].append(img_path.name)\n",
        "            dimension_groups[dim_key]['spacing'].append(spacing)\n",
        "\n",
        "            all_data.append({\n",
        "                'file': img_path.name,\n",
        "                'dataset': 'ImageCHD',\n",
        "                'D': d, 'H': h, 'W': w, 'C': c,\n",
        "                'spacing_x': spacing[0],\n",
        "                'spacing_y': spacing[1],\n",
        "                'spacing_z': spacing[2],\n",
        "                'voxels': d*h*w,\n",
        "                'memory_mb': array.nbytes / (1024**2)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸  Error with {img_path.name}: {e}\")\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DIMENSION GROUPS (D Ã— H Ã— W Ã— Channels)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Sort by total count\n",
        "    sorted_groups = sorted(\n",
        "        dimension_groups.items(),\n",
        "        key=lambda x: len(x[1]['parse22']) + len(x[1]['imagechd']),\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    for i, (dims, data) in enumerate(sorted_groups, 1):\n",
        "        d, h, w, c = dims\n",
        "        p22_count = len(data['parse22'])\n",
        "        ichd_count = len(data['imagechd'])\n",
        "        total = p22_count + ichd_count\n",
        "\n",
        "        # Get spacing statistics\n",
        "        if data['spacing']:\n",
        "            # Average spacing\n",
        "            avg_spacing = tuple(\n",
        "                sum(s[i] for s in data['spacing']) / len(data['spacing'])\n",
        "                for i in range(3)\n",
        "            )\n",
        "\n",
        "            # Most frequent spacing (rounded to 2 decimals)\n",
        "            from collections import Counter\n",
        "            spacing_rounded = [tuple(round(s[i], 2) for i in range(3)) for s in data['spacing']]\n",
        "            most_common_spacing = Counter(spacing_rounded).most_common(1)[0][0]\n",
        "\n",
        "            avg_spacing_str = f\"{avg_spacing[2]:.2f} Ã— {avg_spacing[1]:.2f} Ã— {avg_spacing[0]:.2f} mm\"\n",
        "            mode_spacing_str = f\"{most_common_spacing[2]:.2f} Ã— {most_common_spacing[1]:.2f} Ã— {most_common_spacing[0]:.2f} mm\"\n",
        "        else:\n",
        "            avg_spacing_str = \"N/A\"\n",
        "            mode_spacing_str = \"N/A\"\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Group {i}: {d} Ã— {h} Ã— {w} (channels: {c})\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Total images: {total}\")\n",
        "        print(f\"  â€¢ PARSE22: {p22_count} images\")\n",
        "        print(f\"  â€¢ ImageCHD: {ichd_count} images\")\n",
        "        print(f\"Average spacing: {avg_spacing_str}\")\n",
        "        print(f\"Most frequent spacing: {mode_spacing_str}\")\n",
        "        print(f\"Voxel count: {d * h * w:,}\")\n",
        "\n",
        "        # Show sample files\n",
        "        print(f\"\\nSample files:\")\n",
        "        if data['parse22']:\n",
        "            print(f\"  PARSE22:\")\n",
        "            for name in data['parse22'][:3]:\n",
        "                print(f\"    - {name}\")\n",
        "            if p22_count > 3:\n",
        "                print(f\"    ... and {p22_count - 3} more\")\n",
        "\n",
        "        if data['imagechd']:\n",
        "            print(f\"  ImageCHD:\")\n",
        "            for name in data['imagechd'][:3]:\n",
        "                print(f\"    - {name}\")\n",
        "            if ichd_count > 3:\n",
        "                print(f\"    ... and {ichd_count - 3} more\")\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total images: {len(df)}\")\n",
        "    print(f\"Unique dimension combinations: {len(dimension_groups)}\")\n",
        "\n",
        "    # Dimension statistics by dataset\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"DIMENSION STATISTICS BY DATASET\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    for dataset_name in ['PARSE22', 'ImageCHD']:\n",
        "        dataset_df = df[df['dataset'] == dataset_name]\n",
        "        if len(dataset_df) == 0:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{dataset_name}:\")\n",
        "        print(f\"  Count: {len(dataset_df)} images\")\n",
        "\n",
        "        # Depth (D) statistics\n",
        "        d_mean = dataset_df['D'].mean()\n",
        "        d_mode = dataset_df['D'].mode().values[0] if len(dataset_df['D'].mode()) > 0 else \"N/A\"\n",
        "        d_min = dataset_df['D'].min()\n",
        "        d_max = dataset_df['D'].max()\n",
        "        print(f\"  Depth (D):\")\n",
        "        print(f\"    â€¢ Average: {d_mean:.1f} slices\")\n",
        "        print(f\"    â€¢ Most frequent: {d_mode} slices\")\n",
        "        print(f\"    â€¢ Range: [{d_min}, {d_max}]\")\n",
        "\n",
        "        # Height (H) statistics\n",
        "        h_mean = dataset_df['H'].mean()\n",
        "        h_mode = dataset_df['H'].mode().values[0] if len(dataset_df['H'].mode()) > 0 else \"N/A\"\n",
        "        h_min = dataset_df['H'].min()\n",
        "        h_max = dataset_df['H'].max()\n",
        "        print(f\"  Height (H):\")\n",
        "        print(f\"    â€¢ Average: {h_mean:.1f} pixels\")\n",
        "        print(f\"    â€¢ Most frequent: {h_mode} pixels\")\n",
        "        print(f\"    â€¢ Range: [{h_min}, {h_max}]\")\n",
        "\n",
        "        # Width (W) statistics\n",
        "        w_mean = dataset_df['W'].mean()\n",
        "        w_mode = dataset_df['W'].mode().values[0] if len(dataset_df['W'].mode()) > 0 else \"N/A\"\n",
        "        w_min = dataset_df['W'].min()\n",
        "        w_max = dataset_df['W'].max()\n",
        "        print(f\"  Width (W):\")\n",
        "        print(f\"    â€¢ Average: {w_mean:.1f} pixels\")\n",
        "        print(f\"    â€¢ Most frequent: {w_mode} pixels\")\n",
        "        print(f\"    â€¢ Range: [{w_min}, {w_max}]\")\n",
        "\n",
        "        # Spacing statistics\n",
        "        print(f\"  Spacing:\")\n",
        "        print(f\"    â€¢ X (width) - Avg: {dataset_df['spacing_x'].mean():.3f} mm, Mode: {dataset_df['spacing_x'].mode().values[0]:.3f} mm\")\n",
        "        print(f\"    â€¢ Y (height) - Avg: {dataset_df['spacing_y'].mean():.3f} mm, Mode: {dataset_df['spacing_y'].mode().values[0]:.3f} mm\")\n",
        "        print(f\"    â€¢ Z (depth) - Avg: {dataset_df['spacing_z'].mean():.3f} mm, Mode: {dataset_df['spacing_z'].mode().values[0]:.3f} mm\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"OVERALL STATISTICS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nDimension distribution:\")\n",
        "    print(df.groupby(['D', 'H', 'W']).size().sort_values(ascending=False))\n",
        "\n",
        "    # Memory analysis\n",
        "    print(f\"\\nMemory requirements:\")\n",
        "    print(f\"  Average per image: {df['memory_mb'].mean():.1f} MB\")\n",
        "    print(f\"  Total dataset: {df['memory_mb'].sum():.1f} MB\")\n",
        "    print(f\"  Min: {df['memory_mb'].min():.1f} MB\")\n",
        "    print(f\"  Max: {df['memory_mb'].max():.1f} MB\")\n",
        "\n",
        "    return {\n",
        "        'dimension_groups': dict(dimension_groups),\n",
        "        'dataframe': df,\n",
        "        'summary': {\n",
        "            'total_images': len(df),\n",
        "            'parse22_count': len(df[df['dataset'] == 'PARSE22']),\n",
        "            'imagechd_count': len(df[df['dataset'] == 'ImageCHD']),\n",
        "            'unique_dimensions': len(dimension_groups)\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def print_dimension_lookup(dimension_groups):\n",
        "    \"\"\"\n",
        "    Print a clean lookup table for dimensions.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"QUICK REFERENCE TABLE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    data = []\n",
        "    for dims, info in dimension_groups.items():\n",
        "        d, h, w, c = dims\n",
        "        p22 = len(info['parse22'])\n",
        "        ichd = len(info['imagechd'])\n",
        "        total = p22 + ichd\n",
        "\n",
        "        data.append({\n",
        "            'Dimensions (DÃ—HÃ—W)': f\"{d}Ã—{h}Ã—{w}\",\n",
        "            'PARSE22': p22,\n",
        "            'ImageCHD': ichd,\n",
        "            'Total': total,\n",
        "            'Percentage': f\"{(total/sum(len(i['parse22'])+len(i['imagechd']) for i in dimension_groups.values()))*100:.1f}%\"\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data).sort_values('Total', ascending=False)\n",
        "    print(df.to_string(index=False))\n",
        "    print()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_files_by_dimension(dimension_groups, target_dims):\n",
        "    \"\"\"\n",
        "    Get all files with specific dimensions.\n",
        "\n",
        "    Args:\n",
        "        dimension_groups: Results from quick_dimension_analysis\n",
        "        target_dims: Tuple of (D, H, W, C) or (D, H, W)\n",
        "\n",
        "    Returns:\n",
        "        dict: Files grouped by dataset\n",
        "    \"\"\"\n",
        "    # Handle 3-tuple input\n",
        "    if len(target_dims) == 3:\n",
        "        target_dims = (*target_dims, 1)\n",
        "\n",
        "    if target_dims in dimension_groups:\n",
        "        return dimension_groups[target_dims]\n",
        "    else:\n",
        "        print(f\"Dimension {target_dims} not found!\")\n",
        "        print(\"Available dimensions:\")\n",
        "        for dims in dimension_groups.keys():\n",
        "            print(f\"  {dims}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXAMPLE USAGE IN KAGGLE NOTEBOOK\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set your paths\n",
        "    PARSE22_PATH = \"/kaggle/input/parse-challenge-2022-training-set\"\n",
        "    IMAGECHD_PATH = \"/kaggle/working/unzipped/ImageCHD_dataset\"\n",
        "\n",
        "    # Run analysis\n",
        "    results = quick_dimension_analysis(PARSE22_PATH, IMAGECHD_PATH)\n",
        "\n",
        "    # Print lookup table\n",
        "    #lookup_df = print_dimension_lookup(results['dimension_groups'])\n",
        "\n",
        "    # Save results\n",
        "    #results['dataframe'].to_csv('dataset_dimensions.csv', index=False)\n",
        "    #print(\"ðŸ’¾ Saved: dataset_dimensions.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-08T17:41:04.542348Z",
          "iopub.execute_input": "2025-11-08T17:41:04.551325Z",
          "iopub.status.idle": "2025-11-08T17:45:35.150555Z",
          "shell.execute_reply.started": "2025-11-08T17:41:04.551268Z",
          "shell.execute_reply": "2025-11-08T17:45:35.148777Z"
        },
        "id": "EZRCC9dGNabO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "IY3vO_E2NabO"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}